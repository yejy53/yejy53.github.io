<!-- -- <!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Junyan Ye</title>

    <meta name="author" content="Junyan Ye">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head> -->
<!DOCTYPE HTML>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Junyan Ye</title>

    <meta name="author" content="Junyan Ye">
    <meta name="viewport" content="width=device-width, initial-scale=1">

	
   <style>
	body {
	    font-size: 16px; 
	    font-family: Arial, sans-serif;
	  }
	</style>
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon"
        href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üí°</text></svg>">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-R22F02T1PZ"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());
        gtag('config', 'G-R22F02T1PZ');
    </script>

</head>


<body>
    <table
        style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">

	 	<table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr style="padding:0px">
                                <td style="padding:2.5%;width:63%;vertical-align:middle; font-size: 18px;">
                                    <p style="text-align:center">
                                        <name style="font-size: 28px; display: block; margin-bottom: 15px;">Junyan Ye (Âè∂‰øäË®Ä)</name>
					<p style="font-size:17px;">Greetings! I'm currently a student at Sun Yat-sen University, advised by Prof. <a href="https://liweijia.github.io/" style="font-size:17px">  
					                      Weijia Li</a>.  
					                  I got a B.E. degree at Sun Yat-sen University in 2024.  
					                  Now I'm serving as a research intern at Shanghai AI Lab, and my mentor is <a href="https://lke.cloud.tencent.com/lke#/link-info/redirect?target=https://conghui.github.io/" style="font-size:17px">Conghui He</a>.
							  My ideal is to do influential scientific research!	              
					</p>
                                        My research interest includes:
                                    <ul>
                                       <li style="font-size: 18px;">Remote Sensing & Cross-view tasks</li>
				        <li style="font-size: 18px;">Synthetic Data Detection</li>
				        <li style="font-size: 18px;">large vision-language models</li>
                                    </ul>
<!--                                     <strong>I anticipate graduating in <i>2026</i> for industrial research positions.
                                    If you're interested, please feel free to reach out to me via email or WeChat (xiaoachen98).</strong>
                                    </p> -->
				    <p style="text-align:center; font-size: 19px;">
					    <a href="mailto:yejy53@mail2.sysu.edu.cn">Email</a> / 
					    <a href="https://scholar.google.com.hk/citations?user=6IbGkd4AAAAJ&hl=zh-CN&oi=ao">Google Scholar</a> / 
					    <a href="https://github.com/yejy53">Github</a> / 
					    <a href="https://huggingface.co/Yejy53">Huggingface</a>
				     </p>

                                </td>
<!--                                 <td style="padding:2.5%;width:40%;max-width:40%">
                                    <a href="images/photo_circle_new.png"><img style="width:100%;max-width:100%"
                                            alt="profile photo" src="images/photo_circle_new.png" class="hoverZoomLink"></a>
                                </td> -->
                            </tr>
                        </tbody>
                    </table>



			
		    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
<!--                             <td style="padding:20px;width:100%;vertical-align:middle"> -->
			    <h2 style="font-size:24px; margin-bottom:15px">üìåNews</h2>
		            <p style="font-size:16px; line-height:1.6; margin-top:8px">
				 <p>
                                    [2025.2] üî• Happy to announce that Scene4U was accepted by <strong>CVPR 2025</strong>!
                                </p>  
                                <p>
                                    [2025.2] üéâ Happy to announce that <a href="https://opendatalab.github.io/LOKI/">LOKI</a> was accepted by <strong>ICLR 2025 Spotlight</strong>!
                                </p>  
                                <p>
                                    [2024.12]‚ú® Delighted to announce that <a href="https://opendatalab.github.io/UrBench/">Urbench</a> were accepted by <strong>AAAI 2025</strong>!
                                </p> 
                                <p>
                                    [2024.7] üìú We are very happy that our work <a href="https://github.com/yejy53/EP-BEV">EP-BEV</a> on the cross-view retrieval task was accepted by <strong>ECCV 2024</strong>!
                                </p>                                 
                                <p>
                                    [2024.4] üéâ Our work on cross-view segmentation <a href="https://github.com/yejy53/SG-BEV">SG-BEV</a> was accepted by <strong>CVPR 2024</strong> and selected as the <strong>Highlight poster</strong>!
                                </p>  
				    
<!--                                 <p>
                                    [2024.6] üî• We release <a href="https://sharegpt4video.github.io/">ShareGPT4Video</a>, 
                                    comprising <strong>40K</strong> GPT4V-generated captions, <strong>4.8M</strong> high-quality captions, <strong>a general video captioner</strong>, and <strong>a superior large multi-modal model, ShareGPT4Video-8B</strong>
                                </p>  
                                <p>
                                    [2024.5] We release <a href="https://github.com/xiaoachen98/Open-LLaVA-NeXT">Open-LLaVA-Next</a>, 
                                    an open-source implementation of LLaVA-NeXT series for facilitating the large multi-modal model community. 
                                    All training data and checkpoints at each stage are open-sourced, and friendly for research usage.
                                </p>   
                                <p>
                                    [2024.4] We release <a href="https://arxiv.org/pdf/2403.20330.pdf">MMStar</a>, 
                                    an elite vision-indispensable multi-modal benchmark.
                                </p>                                 
                                <p>
                                    [2024.3] Two papers <a href="https://arxiv.org/pdf/2312.04265.pdf">Rein</a> and
                                     <a href="https://lin-chen.site/projects/freedrag/">FreeDrag</a> were accepted in CVPR 2024!
                                </p>                          -->
                            </td>
                    </table>

		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
		  <tbody>
		    <tr>
		      <td style="padding:20px;width:100%;vertical-align:middle">
		        <h2 style="font-size:24px; margin-bottom:15px">üìñResearch</h2>
		        <p style="font-size:16px; line-height:1.6; margin-top:8px">
		          <strong style="font-size:14px">* indicates the equal contribution.</strong> <br>
			  <strong style="font-size:14px">‚Ä† indicates the corresponding authors .</strong>
		        </p>
		      </td>
		    </tr>
		  </tbody>
		</table>
					
	
		<table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
		        <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/ICLR2025.png" alt="Reins" width="260" height="150">
                                </td>
                                <td width="75%" valign="middle">
                                    <strong>LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models</strong>
                                    <br><br>
                                    <strong><u>Junyan Ye*</u></strong>, Baichuan Zhou*, Zilong Huang*, Junan Zhang*, Tianyi Bai*, Hengrui Kang, Jun He, Honglin Lin, Zihao Wang, Tong Wu, Zhizheng Wu, Yiping Chen, Dahua Lin, Conghui He‚Ä†, Weijia Li‚Ä†
                                    <br><br>
                                    <strong>ICLR 2025, Spotlight</strong>
                                    <br>
                                    [<a href="https://opendatalab.github.io/LOKI/">project page</a>]
                                    [<a href="https://arxiv.org/abs/2410.09732">paper</a>]
                                    [<a href="https://github.com/opendatalab/LOKI">code</a>]
<!--                                     <iframe src="https://ghbtns.com/github-btn.html?user=MMStar-Benchmark&repo=MMStar&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe> -->
<!--                                     <p>We introduce LOKI, a novel benchmark designed to evaluate the ability of large multimodal models (LMMs) to detect synthetic data across various modalities, providing a comprehensive analysis of their performance and limitations in distinguishing real from synthetic content. -->
                                </td>
                            </tr>	

<!--                             <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/ICLR2025.png" alt="Reins" width="260" height="150">
                                </td>
                                <td width="75%" valign="middle">
                                    <strong>Are We on the Right Way for Evaluating Large Vision-Language Models?</strong>
                                    <br>
                                    <strong><u>Lin Chen*</u></strong>, Jinsong Li*, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, Feng Zhao
                                    <br>
                                    <em>NeurIPS, 2024</em>
                                    <br>
                                    [<a href="https://mmstar-benchmark.github.io/">project page</a>]
                                    [<a href="https://arxiv.org/abs/2403.20330">paper</a>]
                                    [<a href="https://github.com/MMStar-Benchmark/MMStar">code</a>]
                                    <iframe src="https://ghbtns.com/github-btn.html?user=MMStar-Benchmark&repo=MMStar&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
                                    <p>We identify two primary issues in existing evaluation studies for large vision-language models. We further
                                        develop an elite vision-indispensable multi-modal benchmark and two novel metrics to measure data leakage and actual performance gain in multi-modal training.
                                    </p>
                                </td>
                            </tr>		 -->

		          <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="data/Urbench.png" alt="Reins" width="260" height="150">
                                </td>
                                <td width="75%" valign="middle">
                                    <strong>Urbench: A Comprehensive Benchmark for Evaluating Large Multimodal Models in Multi-view Urban Scenarios</strong>
                                    <br><br>
                                    Baichuan Zhou*, Haote Yang*, Dairong Chen*, <strong><u>Junyan Ye*</u></strong>, Tianyi Bai, Jinhua Yu, Songyang Zhang, Dahua Lin, <strong>Conghui He‚Ä†</strong>, <strong>Weijia Li‚Ä†</strong>
				    <br><br>
                                    <strong>AAAI 2025</strong>
                                    <br>
                                    [<a href="https://opendatalab.github.io/UrBench/">project page</a>]
                                    [<a href="https://arxiv.org/abs/2408.17267">paper</a>]
                                    [<a href="https://github.com/opendatalab/UrBench">code</a>]
<!--                                     <p>We propose UrBench, a comprehensive benchmark for evaluating Large Multimodal Models in complex multi-view urban scenarios, featuring 11.6K region- and role-level questions across 4 task dimensions (Geo-Localization, Scene Reasoning, Scene Understanding, Object Understanding). -->
                                    </p>
<!-- 				    <br>
				    * Core Contributors.
				    ‚Ä† Corresponding authors -->
                                </td>
                            </tr>	


		  
<!-- <tr style="padding-bottom: 20px;">
  <td style="padding: 10px; vertical-align: middle;">
    <img src="data/Urbench.png" width="160" style="vertical-align: middle;">
  </td>
  <td style="padding: 10px; vertical-align: middle;">
    <a href="https://arxiv.org/abs/2408.17267">
      <span class="papertitle">Urbench: A Comprehensive Benchmark for Evaluating Large Multimodal Models in Multi-view Urban Scenarios</span>
    </a>
    <br>
    Baichuan Zhou*, Haote Yang*, Dairong Chen*, <strong>Junyan Ye*</strong>, Tianyi Bai, Jinhua Yu, Songyang Zhang, Dahua Lin, <strong>Conghui He‚Ä†</strong>, <strong>Weijia Li‚Ä†</strong>
    <br>
    <strong>AAAI 2025<strong>
    <br>
    * Core Contributors.
    ‚Ä† Corresponding authors
  </td>
</tr> -->

		  	 <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/ECCV-home.png" alt="Reins" width="260" height="150">
                                </td>
                                <td width="75%" valign="middle">
                                    <strong>Cross-View Image Geo-Localization with Panorama-BEV Co-Retrieval Network</strong>
                                    <br><br>
                                     <strong><u>Junyan Ye</u></strong>, Zhutao Lv, Weijia Li‚Ä†, Jinhua Yu, Haote Yang, Huaping Zhong, Conghui He‚Ä†
				    <br><br>
                                    <strong>ECCV 2024</strong>
                                    <br>
<!--                                [<a href="https://opendatalab.github.io/UrBench/">project page</a>] -->
                                    [<a href="https://arxiv.org/abs/2408.05475">paper</a>]
                                    [<a href="https://github.com/yejy53/EP-BEV">code</a>]
<!--                                     <p>We propose the Panorama-BEV Co-Retrieval Network for cross-view geolocalization, bridging street-satellite view gaps via geometric BEV transformation and dual-branch retrieval, and introduce the realistic CVGlobal dataset, achieving state-of-the-art performance across multiple benchmarks.                                    </p> -->
                                </td>
                            </tr>	

<!-- <tr style="padding-bottom: 20px;">
  <td style="padding: 10px; vertical-align: middle;">
    <img src="images/ECCV-home.png" width="160" style="vertical-align: middle;">
  </td>
  <td style="padding: 10px; vertical-align: middle;">
    <a href="https://arxiv.org/abs/2408.05475">
      <span class="papertitle">Cross-View Image Geo-Localization with Panorama-BEV Co-Retrieval Network</span>
    </a>
    <br>
    <strong>Junyan Ye</strong>, Zhutao Lv, <strong>Weijia Li*</strong>, Jinhua Yu, Haote Yang, Huaping Zhong, <strong>Conghui He*</strong>
    <br>
    <strong>ECCV 2024</strong>
    <br>
    * Corresponding authors
  </td>
</tr> -->

		  	     <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/CVPR-home.png" alt="Reins" width="260" height="150">
                                </td>
                                <td width="75%" valign="middle">
                                    <strong>SG-BEV: Satellite-Guided BEV Fusion for Cross-View Semantic Segmentation</strong>
                                    <br><br>
                                     <strong><u>Junyan Ye</u></strong>, Qiyan Luo, Jinhua Yu, Huaping Zhong, Zhimeng Zheng, Conghui He, Weijia Li‚Ä†
				    <br><br>
                                    <strong>CVPR 2024 Highlight</strong>
                                    <br>
<!--                                [<a href="https://opendatalab.github.io/UrBench/">project page</a>] -->
                                    [<a href="https://arxiv.org/abs/2404.02638">paper</a>]
                                    [<a href="https://github.com/yejy53/SG-BEV">code</a>]
<!--                                     <p>We propose SG-BEV, a satellite-guided BEV fusion method for cross-view building attribute segmentation, leveraging geometric relations and multi-source data to bridge perspective gaps and optimize feature distribution, achieving state-of-the-art mIOU improvements across multi-city datasets.                                </td> -->
                            </tr>	

<!-- <tr style="padding-bottom: 20px;">
  <td style="padding: 10px; vertical-align: middle;">
    <img src='images/CVPR-home.png' width="160" style="vertical-align:middle;">
  </td>
  <td style="padding: 10px; vertical-align: middle;">
    <a href="https://arxiv.org/abs/2404.02638">
      <span class="papertitle">SG-BEV: Satellite-Guided BEV Fusion for Cross-View Semantic Segmentation</span>
    </a>
    <br>
    <strong>Junyan Ye</strong>, Qiyan Luo, Jinhua Yu, Huaping Zhong, Zhimeng Zheng, Conghui He, <strong>Weijia Li*</strong>
    <br>
    <strong>CVPR 2024 Highlight</strong>
  </td>
</tr> -->


		    <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/Crossviewdiff-home.png" alt="Reins" width="260" height="150">
                                </td>
                                <td width="75%" valign="middle">
                                    <strong>CrossViewDiff: A Cross-View Diffusion Model for Satellite-to-Street View Synthesis</strong>
                                    <br><br>
				    Weijia Li*, Jun He*, <strong><u>Junyan Ye*</u></strong>,Huaping Zhong*, Zhimeng Zheng, Zilong Huang, Dahua Lin, Conghui He‚Ä†
                                    [<a href="https://opendatalab.github.io/skydiffusion/">project page</a>]
                                    [<a href="https://arxiv.org/abs/2408.01812">paper</a>]
                                    [<a href="https://github.com/opendatalab/skydiffusion">code</a>]
                            </tr>	
				
			    <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/Skydiffusion.png" alt="Reins" width="260" height="150">
                                </td><br>
                                <td width="75%" valign="middle">
                                    <strong>SkyDiffusion: Street-to-Satellite Image Synthesis with Diffusion Models and BEV Paradigm</strong>
                                    <br><br>
                                     <strong><u>Junyan Ye</u></strong>,Jun He*, Weijia Li‚Ä†, Zhutao Lv, Jinhua Yu, Haote Yang, Conghui He‚Ä†
                                    [<a href="https://opendatalab.github.io/skydiffusion/">project page</a>]
                                    [<a href="https://arxiv.org/abs/2408.01812">paper</a>]
                                    [<a href="https://github.com/opendatalab/skydiffusion">code</a>]
<!--                                     <p>We propose SG-BEV, a satellite-guided BEV fusion method for cross-view building attribute segmentation, leveraging geometric relations and multi-source data to bridge perspective gaps and optimize feature distribution, achieving state-of-the-art mIOU improvements across multi-city datasets.                                </td> -->
                            </tr>	
		
<!-- 				<tr style="padding-bottom: 20px;">
				  <td style="padding: 10px; vertical-align: middle;">
				    <img src="images/skydiff-home.png" width="160" style="vertical-align: middle;">
				  </td>
				  <td style="padding: 10px; vertical-align: middle;">
				    <a href="https://opendatalab.github.io/skydiffusion/">
				      <span class="papertitle">SkyDiffusion: Street-to-Satellite Image Synthesis with Diffusion Models and BEV Paradigm</span>
				    </a>
				    <br>
				    <strong>Junyan Ye*</strong>, Jun He*, <strong>Weijia Li‚Ä†</strong>, Zhutao Lv, Jinhua Yu, Haote Yang, <strong>Conghui He‚Ä†</strong>
				    <br>
				    <em>arXiv preprint</em>
				    <br>
				    * Core Contributors.
				    ‚Ä† Corresponding authors
				  </td>
				</tr> -->


			
<!-- 				<tr style="padding-bottom: 20px;">
				  <td style="padding: 10px; vertical-align: middle;">
				    <img src="images/Crossviewdiff-home.png" width="160" style="vertical-align: middle;">
				  </td>
				  <td style="padding: 10px; vertical-align: middle;">
				    <a href="https://www.arxiv.org/abs/2408.14765">
				      <span class="papertitle">CrossViewDiff: A Cross-View Diffusion Model for Satellite-to-Street View Synthesis</span>
				    </a>
				    <br>
				    Weijia Li*, Jun He*, <strong>Junyan Ye*</strong>,Huaping Zhong*, Zhimeng Zheng, Zilong Huang, Dahua Lin, <strong>Conghui He‚Ä†</strong>
				    <br>
				    <em>arXiv preprint</em>
				    <br>
				    * Core Contributors.
				    ‚Ä† Corresponding authors
				  </td>
				</tr> -->

			    <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/RS-home.png" alt="Reins" width="260" height="150">
                                </td>
                                <td width="75%" valign="middle">
                                    <strong>Corn Phenology Detection Using the Derivative Dynamic Time Warping Method and Sentinel-2 Time Series</strong>
                                    <br><br>
				    <strong><u>Junyan Ye*</u></strong>, Wenhao Bao, Chunhua Liao‚Ä†, Dairong Chen, Haoxuan Hu
<!--                                     [<a href="https://opendatalab.github.io/skydiffusion/">project page</a>] -->
                                    [<a href="https://www.mdpi.com/2072-4292/15/14/3456">paper</a>]
<!--                                     [<a href="https://github.com/opendatalab/skydiffusion">code</a>] -->
                            </tr>

                        </tbody>
                    </table>

		<table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin:auto">
		  <tbody>
		    <td style="padding:20px;width:100%;vertical-align:middle">
		      
		      <heading style="font-size:1.5em; border-bottom:2px solid #ddd; padding-bottom:8px">
		        üìù Academic Service (Reviewer)
		      </heading>
		      
		      
		      <ul style="font-size:1.2em; margin-top:15px; list-style:none; padding-left:0">
		        <li style="margin:12px 0"><strong>ICCV</strong> 2025</li>
		        <li style="margin:12px 0"><strong>CVPR</strong> 2025</li>
		        <li style="margin:12px 0"><strong>TCSVT</strong></li>
		      </ul>
		    </td>
		  </tbody>
		</table>
	        <table width="100%" cellspacing="0" cellpadding="20" border="0" align="center">
	        <tbody>
	            <tr>
	                <td>
	                    <br>
	                    <p align="right">
	                        <font size="1">
	                            Thanks the original template from <a href="https://jonbarron.info/">jonbarron</a> and the
	                            modifications made by <a href="https://yujun-shi.github.io/">shi</a>.
	                        </font>
	                    </p>
	                </td>
	            </tr>
	        </tbody>
	    </table>
	    </td>
	    </tr>
	    </table>
  </body>
</html>
 -->
